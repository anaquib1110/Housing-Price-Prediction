{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA Available:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers pandas numpy scikit-learn torch matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel, RobertaTokenizer, RobertaModel\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, RobertaTokenizer, RobertaModel\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Explore the data\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(\"\\nSample data:\")\n",
    "print(train_df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# Fill missing values\n",
    "train_df['location'] = train_df['location'].fillna(\"unknown\")\n",
    "train_df['keyword'] = train_df['keyword'].fillna(\"unknown\")\n",
    "test_df['location'] = test_df['location'].fillna(\"unknown\")\n",
    "test_df['keyword'] = test_df['keyword'].fillna(\"unknown\")\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        # Remove user mentions\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        # Remove hashtags symbol (but keep the text)\n",
    "        text = re.sub(r'#', '', text)\n",
    "        # Remove extra spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    return \"empty\"\n",
    "\n",
    "train_df['clean_text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['clean_text'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Create train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[['clean_text', 'keyword', 'location']], \n",
    "    train_df['target'], \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=train_df['target']\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last improvement.\n",
    "            verbose (bool): If True, prints a message for each improvement.\n",
    "            delta (float): Minimum change to qualify as an improvement.\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0 \n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss  # Higher score is better\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "            \n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Save model when validation loss decreases.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# ---------------------- ADD THIS NEW FUNCTION FOR CLASS WEIGHTS ----------------------\n",
    "# This helps address class imbalance by giving more weight to the minority class\n",
    "def calculate_class_weights(y_train):\n",
    "    class_counts = np.bincount(y_train)\n",
    "    total_samples = len(y_train)\n",
    "    class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "    return torch.FloatTensor(class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- REPLACE YOUR DATASET CLASS WITH THIS ENHANCED VERSION ----------------------\n",
    "class HybridBERTRoBERTaDataset(Dataset):\n",
    "    def __init__(self, texts, keywords, locations, targets=None, bert_tokenizer=None, roberta_tokenizer=None, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.keywords = keywords\n",
    "        self.locations = locations\n",
    "        self.targets = targets\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.roberta_tokenizer = roberta_tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        keyword = self.keywords.iloc[idx]\n",
    "        location = self.locations.iloc[idx]\n",
    "        \n",
    "        # IMPROVED: Enhanced text combining - using special separators for better context\n",
    "        # This helps the model understand the different parts of the input better\n",
    "        combined_text = f\"{text} [SEP] keyword: {keyword} [SEP] location: {location}\"\n",
    "        \n",
    "        # BERT encoding\n",
    "        bert_encoding = self.bert_tokenizer.encode_plus(\n",
    "            combined_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # RoBERTa encoding\n",
    "        roberta_encoding = self.roberta_tokenizer.encode_plus(\n",
    "            combined_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            # RoBERTa doesn't use token_type_ids\n",
    "            return_token_type_ids=False,   \n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'bert_input_ids': bert_encoding['input_ids'].flatten(),\n",
    "            'bert_attention_mask': bert_encoding['attention_mask'].flatten(),\n",
    "            'bert_token_type_ids': bert_encoding['token_type_ids'].flatten(),\n",
    "            'roberta_input_ids': roberta_encoding['input_ids'].flatten(),\n",
    "            'roberta_attention_mask': roberta_encoding['attention_mask'].flatten(),\n",
    "        }\n",
    "        \n",
    "        if self.targets is not None:\n",
    "            item['targets'] = torch.tensor(self.targets.iloc[idx], dtype=torch.long)\n",
    "            \n",
    "        return item\n",
    "\n",
    "class HybridBERTRoBERTaModel(nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased', roberta_model_name='roberta-base', num_classes=2, dropout_rate=0.3):\n",
    "        super(HybridBERTRoBERTaModel, self).__init__()\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.bert_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "        self.roberta_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        bert_hidden_size = self.bert.config.hidden_size\n",
    "        roberta_hidden_size = self.roberta.config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size + roberta_hidden_size, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(768, 384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(384, num_classes)\n",
    "        )\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size + roberta_hidden_size, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def mean_pooling(self, last_hidden_state, attention_mask):\n",
    "        # last_hidden_state: (batch_size, seq_len, hidden_size)\n",
    "        # attention_mask: (batch_size, seq_len)\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        return sum_embeddings / torch.clamp(sum_mask, min=1e-9)\n",
    "\n",
    "    def forward(self, bert_input_ids, bert_attention_mask, bert_token_type_ids,\n",
    "                roberta_input_ids, roberta_attention_mask):\n",
    "\n",
    "        # BERT output with mean pooling\n",
    "        bert_outputs = self.bert(\n",
    "            input_ids=bert_input_ids,\n",
    "            attention_mask=bert_attention_mask,\n",
    "            token_type_ids=bert_token_type_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "        bert_mean_pooled = self.mean_pooling(bert_outputs.last_hidden_state, bert_attention_mask)\n",
    "        bert_mean_pooled = self.bert_dropout(bert_mean_pooled)\n",
    "\n",
    "        # RoBERTa output with mean pooling\n",
    "        roberta_outputs = self.roberta(\n",
    "            input_ids=roberta_input_ids,\n",
    "            attention_mask=roberta_attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        roberta_mean_pooled = self.mean_pooling(roberta_outputs.last_hidden_state, roberta_attention_mask)\n",
    "        roberta_mean_pooled = self.roberta_dropout(roberta_mean_pooled)\n",
    "\n",
    "        # Concatenate and apply attention\n",
    "        concatenated = torch.cat((bert_mean_pooled, roberta_mean_pooled), dim=1)\n",
    "        attention_weights = self.attention(concatenated)\n",
    "        weighted_output = concatenated * attention_weights\n",
    "\n",
    "        logits = self.classifier(weighted_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, device, epochs=3):\n",
    "    # IMPROVED: Add early stopping\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True, path='best_hybrid_model.pt')\n",
    "    \n",
    "    # IMPROVED: Calculate class weights to handle class imbalance\n",
    "    targets = []\n",
    "    for batch in train_dataloader:\n",
    "        targets.extend(batch['targets'].numpy())\n",
    "    class_weights = calculate_class_weights(targets)\n",
    "    class_weights = class_weights.to(device)\n",
    "    \n",
    "    # IMPROVED: Use weighted loss function\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    # Training metrics history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_precision': [],\n",
    "        'val_recall': [],\n",
    "        'val_f1': [],\n",
    "        'val_f1_disaster': []  # IMPROVED: Track F1 specifically for the disaster class\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        print(\"-\" * 10)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        from tqdm import tqdm \n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            bert_input_ids = batch['bert_input_ids'].to(device)\n",
    "            bert_attention_mask = batch['bert_attention_mask'].to(device)\n",
    "            bert_token_type_ids = batch['bert_token_type_ids'].to(device)\n",
    "            roberta_input_ids = batch['roberta_input_ids'].to(device)\n",
    "            roberta_attention_mask = batch['roberta_attention_mask'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                bert_input_ids=bert_input_ids,\n",
    "                bert_attention_mask=bert_attention_mask,\n",
    "                bert_token_type_ids=bert_token_type_ids,\n",
    "                roberta_input_ids=roberta_input_ids,\n",
    "                roberta_attention_mask=roberta_attention_mask\n",
    "            )\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        epoch_train_loss = running_loss / len(train_dataloader)\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        \n",
    "        print(f\"Training Loss: {epoch_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                bert_input_ids = batch['bert_input_ids'].to(device)\n",
    "                bert_attention_mask = batch['bert_attention_mask'].to(device)\n",
    "                bert_token_type_ids = batch['bert_token_type_ids'].to(device)\n",
    "                roberta_input_ids = batch['roberta_input_ids'].to(device)\n",
    "                roberta_attention_mask = batch['roberta_attention_mask'].to(device)\n",
    "                targets = batch['targets'].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    bert_input_ids=bert_input_ids,\n",
    "                    bert_attention_mask=bert_attention_mask,\n",
    "                    bert_token_type_ids=bert_token_type_ids,\n",
    "                    roberta_input_ids=roberta_input_ids,\n",
    "                    roberta_attention_mask=roberta_attention_mask\n",
    "                )\n",
    "                \n",
    "                loss = criterion(outputs, targets)\n",
    "                val_running_loss += loss.item()\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        epoch_val_loss = val_running_loss / len(val_dataloader)\n",
    "        epoch_val_accuracy = accuracy_score(all_targets, all_preds)\n",
    "        \n",
    "        # IMPROVED: Get detailed metrics with class-specific results\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_preds, average='weighted')\n",
    "        \n",
    "        # IMPROVED: Get F1 score specifically for the disaster class (class 1)\n",
    "        _, _, f1_disaster, _ = precision_recall_fscore_support(all_targets, all_preds, average=None)\n",
    "        \n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_accuracy'].append(epoch_val_accuracy)\n",
    "        history['val_precision'].append(precision)\n",
    "        history['val_recall'].append(recall)\n",
    "        history['val_f1'].append(f1)\n",
    "        history['val_f1_disaster'].append(f1_disaster[1] if len(f1_disaster) > 1 else 0)\n",
    "        \n",
    "        print(f\"Validation Loss: {epoch_val_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {epoch_val_accuracy:.4f}\")\n",
    "        print(f\"Validation Precision: {precision:.4f}\")\n",
    "        print(f\"Validation Recall: {recall:.4f}\")\n",
    "        print(f\"Validation F1: {f1:.4f}\")\n",
    "        print(f\"Validation F1 (Disaster class): {f1_disaster[1] if len(f1_disaster) > 1 else 0:.4f}\")\n",
    "        \n",
    "        # IMPROVED: Early stopping check\n",
    "        early_stopping(epoch_val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "            \n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, test_dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            bert_input_ids = batch['bert_input_ids'].to(device)\n",
    "            bert_attention_mask = batch['bert_attention_mask'].to(device)\n",
    "            bert_token_type_ids = batch['bert_token_type_ids'].to(device)\n",
    "            roberta_input_ids = batch['roberta_input_ids'].to(device)\n",
    "            roberta_attention_mask = batch['roberta_attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                bert_input_ids=bert_input_ids,\n",
    "                bert_attention_mask=bert_attention_mask,\n",
    "                bert_token_type_ids=bert_token_type_ids,\n",
    "                roberta_input_ids=roberta_input_ids,\n",
    "                roberta_attention_mask=roberta_attention_mask\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot precision and recall\n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.plot(history['val_precision'], label='Validation Precision')\n",
    "    plt.plot(history['val_recall'], label='Validation Recall')\n",
    "    plt.title('Precision and Recall')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot F1 score\n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.plot(history['val_f1'], label='Validation F1 (Overall)')\n",
    "    if 'val_f1_disaster' in history:\n",
    "        plt.plot(history['val_f1_disaster'], label='Validation F1 (Disaster Class)', linestyle='--')\n",
    "    plt.title('F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    # IMPROVED: Add learning curves\n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.plot(history['train_loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if device.type == \"cpu\":\n",
    "        print(\"GPU not available, using CPU.\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # No of Epochs - INCREASED for better learning\n",
    "    EPOCHS = 25\n",
    "    \n",
    "    # Initialize tokenizers\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = HybridBERTRoBERTaDataset(\n",
    "        texts=X_train['clean_text'],\n",
    "        keywords=X_train['keyword'],\n",
    "        locations=X_train['location'],\n",
    "        targets=y_train,\n",
    "        bert_tokenizer=bert_tokenizer,\n",
    "        roberta_tokenizer=roberta_tokenizer\n",
    "    )\n",
    "    \n",
    "    val_dataset = HybridBERTRoBERTaDataset(\n",
    "        texts=X_val['clean_text'],\n",
    "        keywords=X_val['keyword'],\n",
    "        locations=X_val['location'],\n",
    "        targets=y_val,\n",
    "        bert_tokenizer=bert_tokenizer,\n",
    "        roberta_tokenizer=roberta_tokenizer\n",
    "    )\n",
    "    \n",
    "    test_dataset = HybridBERTRoBERTaDataset(\n",
    "        texts=test_df['clean_text'],\n",
    "        keywords=test_df['keyword'],\n",
    "        locations=test_df['location'],\n",
    "        targets=None,\n",
    "        bert_tokenizer=bert_tokenizer,\n",
    "        roberta_tokenizer=roberta_tokenizer\n",
    "    )\n",
    "    \n",
    "    # IMPROVED: Adjusted batch sizes for better training\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Initialize model with improved architecture\n",
    "    model = HybridBERTRoBERTaModel()\n",
    "    model.to(device)\n",
    "    \n",
    "    # IMPROVED: Better optimizer settings\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01, eps=1e-8)\n",
    "    \n",
    "    # IMPROVED: Use proper learning rate scheduler with warmup\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "    warmup_steps = int(0.1 * total_steps)  # 10% of total steps for warmup\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=warmup_steps, \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model, history = train_model(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,   \n",
    "        epochs=EPOCHS\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_hybrid_model.pt'))\n",
    "    \n",
    "    # Get predictions on validation set for detailed evaluation\n",
    "    val_predictions = evaluate_model(model, val_dataloader, device)\n",
    "    print(\"\\nDetailed Validation Metrics:\")\n",
    "    print(classification_report(y_val, val_predictions))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_val, val_predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # IMPROVED: Analyze specific misclassifications\n",
    "    misclassified_indices = np.where(np.array(val_predictions) != np.array(y_val))[0]\n",
    "    if len(misclassified_indices) > 0:\n",
    "        print(\"\\nSample of misclassified tweets:\")\n",
    "        sample_size = min(10, len(misclassified_indices))\n",
    "        sampled_indices = np.random.choice(misclassified_indices, sample_size, replace=False)\n",
    "        \n",
    "        for idx in sampled_indices:\n",
    "            text = X_val['clean_text'].iloc[idx]\n",
    "            true_label = y_val.iloc[idx]\n",
    "            pred_label = val_predictions[idx]\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"True: {true_label}, Predicted: {pred_label}\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    # Get predictions on test set\n",
    "    test_predictions = evaluate_model(model, test_dataloader, device)\n",
    "    \n",
    "    # Create submission file\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'target': test_predictions\n",
    "    })\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(\"\\nTest predictions saved to 'submission.csv'\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
